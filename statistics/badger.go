/*
 * @Author: kamalyes 501893067@qq.com
 * @Date: 2026-01-26 00:00:00
 * @LastEditors: kamalyes 501893067@qq.com
 * @LastEditTime: 2026-01-26 00:00:00
 * @FilePath: \go-stress\statistics\badger.go
 * @Description: BadgerDB å­˜å‚¨é€‚é…å™¨ - é«˜æ€§èƒ½ LSM-Tree å­˜å‚¨
 *
 * Copyright (c) 2026 by kamalyes, All Rights Reserved.
 */
package statistics

import (
	"encoding/json"
	"fmt"
	"strings"
	"sync"
	"time"

	"github.com/dgraph-io/badger/v4"
	"github.com/kamalyes/go-logger"
	"github.com/kamalyes/go-toolbox/pkg/syncx"
)

// BadgerStorage BadgerDB å­˜å‚¨ï¼ˆå®ç° DetailStorageInterfaceï¼‰
type BadgerStorage struct {
	db        *badger.DB
	writeChan chan *RequestResult
	batchSize int
	wg        sync.WaitGroup
	mu        sync.RWMutex
	nodeID    string
	logger    logger.ILogger
	closed    bool

	// å®æ—¶è®¡æ•°å™¨
	totalCount   *syncx.Uint64
	successCount *syncx.Uint64
	failedCount  *syncx.Uint64
	skippedCount *syncx.Uint64
}

// NewBadgerStorage åˆ›å»º BadgerDB å­˜å‚¨
func NewBadgerStorage(dbPath, nodeID string, log logger.ILogger) (*BadgerStorage, error) {
	log.Infof("ğŸ—„ï¸  åˆå§‹åŒ– BadgerDB å­˜å‚¨: %s (èŠ‚ç‚¹: %s)", dbPath, nodeID)

	// BadgerDB é…ç½®
	opts := badger.DefaultOptions(dbPath).
		WithLoggingLevel(badger.WARNING). // å‡å°‘æ—¥å¿—
		WithNumVersionsToKeep(1).         // åªä¿ç•™æœ€æ–°ç‰ˆæœ¬
		WithCompactL0OnClose(true).       // å…³é—­æ—¶å‹ç¼©
		WithValueThreshold(256).          // å¤§äº 256 å­—èŠ‚çš„å€¼å•ç‹¬å­˜å‚¨
		WithNumMemtables(2).              // å†…å­˜è¡¨æ•°é‡
		WithNumLevelZeroTables(2).        // L0 è¡¨æ•°é‡
		WithNumLevelZeroTablesStall(4).   // L0 è¡¨åœé¡¿é˜ˆå€¼
		WithMaxLevels(5).                 // æœ€å¤§å±‚çº§
		WithValueLogFileSize(64 << 20).   // 64MB value log
		WithBlockCacheSize(64 << 20).     // 64MB block cache
		WithIndexCacheSize(32 << 20).     // 32MB index cache
		WithSyncWrites(false).            // å¼‚æ­¥å†™å…¥ï¼ˆæ€§èƒ½ä¼˜å…ˆï¼‰
		WithDetectConflicts(false).       // ç¦ç”¨å†²çªæ£€æµ‹
		WithNumCompactors(2)              // å‹ç¼©çº¿ç¨‹æ•°

	db, err := badger.Open(opts)
	if err != nil {
		return nil, fmt.Errorf("æ‰“å¼€ BadgerDB å¤±è´¥: %w", err)
	}

	log.Infof("âœ… BadgerDB å·²å¯åŠ¨ (èŠ‚ç‚¹: %s, è·¯å¾„: %s)", nodeID, dbPath)

	storage := &BadgerStorage{
		db:           db,
		writeChan:    make(chan *RequestResult, 10000), // 1ä¸‡ç¼“å†²
		batchSize:    500,                              // æ¯æ‰¹ 500 æ¡
		nodeID:       nodeID,
		logger:       log,
		closed:       false,
		totalCount:   syncx.NewUint64(0),
		successCount: syncx.NewUint64(0),
		failedCount:  syncx.NewUint64(0),
		skippedCount: syncx.NewUint64(0),
	}

	// å¯åŠ¨æ‰¹é‡å†™å…¥åç¨‹
	storage.wg.Add(1)
	go storage.batchWriter()

	// å¯åŠ¨åå°GC
	storage.wg.Add(1)
	go storage.runGC()

	return storage, nil
}

// Write å¼‚æ­¥å†™å…¥è¯·æ±‚è¯¦æƒ…
func (s *BadgerStorage) Write(detail *RequestResult) {
	s.mu.Lock()
	if s.closed {
		s.mu.Unlock()
		return
	}
	s.mu.Unlock()

	select {
	case s.writeChan <- detail:
		// æˆåŠŸå…¥é˜Ÿ
	default:
		// é˜Ÿåˆ—æ»¡ï¼ŒåŒæ­¥å†™å…¥ï¼ˆé¿å…ä¸¢æ•°æ®ï¼‰
		s.logger.Warnf("âš ï¸  å†™å…¥é˜Ÿåˆ—å·²æ»¡ï¼ŒåŒæ­¥å†™å…¥: %s", detail.ID)
		s.writeOne(detail)
	}
}

// batchWriter æ‰¹é‡å†™å…¥åç¨‹
func (s *BadgerStorage) batchWriter() {
	defer s.wg.Done()

	batch := make([]*RequestResult, 0, s.batchSize)
	ticker := time.NewTicker(1 * time.Second) // æ¯ç§’åˆ·æ–°
	defer ticker.Stop()

	flush := func() {
		if len(batch) == 0 {
			return
		}

		if err := s.writeBatch(batch); err != nil {
			s.logger.Errorf("âŒ BadgerDB æ‰¹é‡å†™å…¥å¤±è´¥: %v", err)
		} else {
			s.logger.Debugf("âœ… BadgerDB æ‰¹é‡å†™å…¥æˆåŠŸ: %d æ¡", len(batch))
		}

		batch = batch[:0] // æ¸…ç©ºä½†ä¿ç•™å®¹é‡
	}

	for {
		select {
		case detail, ok := <-s.writeChan:
			if !ok {
				flush()
				return
			}

			batch = append(batch, detail)

			// è¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œç«‹å³å†™å…¥
			if len(batch) >= s.batchSize {
				flush()
			}

		case <-ticker.C:
			// å®šæ—¶åˆ·æ–°
			flush()
		}
	}
}

// writeOne åŒæ­¥å†™å…¥å•æ¡
func (s *BadgerStorage) writeOne(detail *RequestResult) error {
	return s.db.Update(func(txn *badger.Txn) error {
		key := s.makeKey(detail)
		value, err := json.Marshal(detail)
		if err != nil {
			return err
		}

		if err := txn.Set(key, value); err != nil {
			return err
		}

		// æ›´æ–°è®¡æ•°å™¨
		s.totalCount.Add(1)
		if detail.Skipped {
			s.skippedCount.Add(1)
		} else if detail.Success {
			s.successCount.Add(1)
		} else {
			s.failedCount.Add(1)
		}

		return nil
	})
}

// writeBatch æ‰¹é‡å†™å…¥
func (s *BadgerStorage) writeBatch(batch []*RequestResult) error {
	wb := s.db.NewWriteBatch()
	defer wb.Cancel()

	for _, detail := range batch {
		key := s.makeKey(detail)
		value, err := json.Marshal(detail)
		if err != nil {
			s.logger.Errorf("âŒ åºåˆ—åŒ–å¤±è´¥: %v", err)
			continue
		}

		if err := wb.Set(key, value); err != nil {
			s.logger.Errorf("âŒ å†™å…¥å¤±è´¥: %v", err)
			continue
		}

		// æ›´æ–°è®¡æ•°å™¨
		s.totalCount.Add(1)
		if detail.Skipped {
			s.skippedCount.Add(1)
		} else if detail.Success {
			s.successCount.Add(1)
		} else {
			s.failedCount.Add(1)
		}
	}

	return wb.Flush()
}

// makeKey ç”Ÿæˆå­˜å‚¨é”®
// æ ¼å¼: req:{nodeID}:{taskID}:{timestamp}:{id}
func (s *BadgerStorage) makeKey(detail *RequestResult) []byte {
	return []byte(fmt.Sprintf("req:%s:%s:%d:%s",
		detail.NodeID,
		detail.TaskID,
		detail.Timestamp.Unix(),
		detail.ID,
	))
}

// Query åˆ†é¡µæŸ¥è¯¢è¯·æ±‚è¯¦æƒ…
func (s *BadgerStorage) Query(offset, limit int, statusFilter StatusFilter, nodeID, taskID string) ([]*RequestResult, error) {
	results := make([]*RequestResult, 0, limit)

	err := s.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchSize = limit * 2
		opts.Reverse = true // å€’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰

		it := txn.NewIterator(opts)
		defer it.Close()

		// æ„å»ºå‰ç¼€
		prefix := s.makePrefix(nodeID, taskID)
		skipped := 0
		matched := 0

		for it.Seek([]byte(prefix + "\xff")); it.ValidForPrefix([]byte(prefix)); it.Next() {
			item := it.Item()

			// è·å–å€¼
			var detail RequestResult
			err := item.Value(func(val []byte) error {
				return json.Unmarshal(val, &detail)
			})
			if err != nil {
				s.logger.Errorf("âŒ ååºåˆ—åŒ–å¤±è´¥: %v", err)
				continue
			}

			// çŠ¶æ€è¿‡æ»¤
			if !s.matchFilter(&detail, statusFilter) {
				continue
			}

			// è·³è¿‡ offset
			if skipped < offset {
				skipped++
				continue
			}

			// è¾¾åˆ° limitï¼Œåœæ­¢
			if matched >= limit {
				break
			}

			results = append(results, &detail)
			matched++
		}

		return nil
	})

	return results, err
}

// Count ç»Ÿè®¡æ€»æ•°
func (s *BadgerStorage) Count(statusFilter StatusFilter, nodeID, taskID string) (int, error) {
	// å¦‚æœæ²¡æœ‰è¿‡æ»¤æ¡ä»¶ï¼Œç›´æ¥è¿”å›è®¡æ•°å™¨
	if nodeID == "" && taskID == "" {
		switch statusFilter {
		case StatusFilterSuccess:
			return int(s.successCount.Load()), nil
		case StatusFilterFailed:
			return int(s.failedCount.Load()), nil
		case StatusFilterSkipped:
			return int(s.skippedCount.Load()), nil
		case StatusFilterAll:
			return int(s.totalCount.Load()), nil
		}
	}

	// æœ‰è¿‡æ»¤æ¡ä»¶ï¼Œéœ€è¦éå†ç»Ÿè®¡
	count := 0

	err := s.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = false // åªéœ€è¦é”®

		it := txn.NewIterator(opts)
		defer it.Close()

		prefix := s.makePrefix(nodeID, taskID)

		for it.Seek([]byte(prefix)); it.ValidForPrefix([]byte(prefix)); it.Next() {
			item := it.Item()

			// éœ€è¦è·å–å€¼æ¥åˆ¤æ–­çŠ¶æ€
			var detail RequestResult
			err := item.Value(func(val []byte) error {
				return json.Unmarshal(val, &detail)
			})
			if err != nil {
				continue
			}

			if s.matchFilter(&detail, statusFilter) {
				count++
			}
		}

		return nil
	})

	return count, err
}

// makePrefix ç”ŸæˆæŸ¥è¯¢å‰ç¼€
func (s *BadgerStorage) makePrefix(nodeID, taskID string) string {
	if nodeID == "" && taskID == "" {
		return "req:"
	}
	if taskID == "" {
		return fmt.Sprintf("req:%s:", nodeID)
	}
	return fmt.Sprintf("req:%s:%s:", nodeID, taskID)
}

// matchFilter åŒ¹é…çŠ¶æ€è¿‡æ»¤å™¨
func (s *BadgerStorage) matchFilter(detail *RequestResult, filter StatusFilter) bool {
	switch filter {
	case StatusFilterSuccess:
		return detail.Success && !detail.Skipped
	case StatusFilterFailed:
		return !detail.Success && !detail.Skipped
	case StatusFilterSkipped:
		return detail.Skipped
	case StatusFilterAll:
		return true
	default:
		return true
	}
}

// Close å…³é—­å­˜å‚¨
func (s *BadgerStorage) Close() error {
	s.mu.Lock()
	if s.closed {
		s.mu.Unlock()
		return nil
	}
	s.closed = true
	s.mu.Unlock()

	s.logger.Info("ğŸ”’ å…³é—­ BadgerDB å­˜å‚¨...")

	close(s.writeChan)
	s.wg.Wait()

	return s.db.Close()
}

// GetNodeID è·å–èŠ‚ç‚¹ID
func (s *BadgerStorage) GetNodeID() string {
	return s.nodeID
}

// GetStats è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
func (s *BadgerStorage) GetStats() map[string]interface{} {
	lsm, vlog := s.db.Size()

	return map[string]interface{}{
		"type":          "badger",
		"node_id":       s.nodeID,
		"total_count":   s.totalCount.Load(),
		"success_count": s.successCount.Load(),
		"failed_count":  s.failedCount.Load(),
		"skipped_count": s.skippedCount.Load(),
		"lsm_size":      lsm,
		"vlog_size":     vlog,
		"total_size":    lsm + vlog,
	}
}

// runGC åå°åƒåœ¾å›æ”¶
func (s *BadgerStorage) runGC() {
	defer s.wg.Done()

	ticker := time.NewTicker(5 * time.Minute)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			s.mu.RLock()
			if s.closed {
				s.mu.RUnlock()
				return
			}
			s.mu.RUnlock()

			// è¿è¡Œ GC
			err := s.db.RunValueLogGC(0.5) // å›æ”¶ 50% ä»¥ä¸Šç©ºé—´çš„æ—¥å¿—æ–‡ä»¶
			if err != nil && !strings.Contains(err.Error(), "nothing to GC") {
				s.logger.Warnf("âš ï¸  BadgerDB GC è­¦å‘Š: %v", err)
			}
		}
	}
}
