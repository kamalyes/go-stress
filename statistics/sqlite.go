/*
 * @Author: kamalyes 501893067@qq.com
 * @Date: 2026-01-24 00:00:00
 * @LastEditors: kamalyes 501893067@qq.com
 * @LastEditTime: 2026-01-24 16:00:00
 * @FilePath: \go-stress\statistics\sqlite.go
 * @Description: SQLiteå­˜å‚¨å±‚ - æŒä¹…åŒ–è¯·æ±‚è¯¦æƒ…ï¼ˆæ”¯æŒæ— é™å­˜å‚¨ï¼‰
 *
 * Copyright (c) 2026 by kamalyes, All Rights Reserved.
 */
package statistics

import (
	"database/sql"
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/kamalyes/go-logger"
	_ "github.com/mattn/go-sqlite3"
)

// StatusFilter çŠ¶æ€è¿‡æ»¤å™¨æšä¸¾
type StatusFilter int

const (
	StatusFilterAll     StatusFilter = iota // å…¨éƒ¨
	StatusFilterSuccess                     // æˆåŠŸ
	StatusFilterFailed                      // å¤±è´¥
	StatusFilterSkipped                     // è·³è¿‡
)

// String è¿”å›çŠ¶æ€è¿‡æ»¤å™¨çš„å­—ç¬¦ä¸²è¡¨ç¤º
func (s StatusFilter) String() string {
	switch s {
	case StatusFilterSuccess:
		return "success"
	case StatusFilterFailed:
		return "failed"
	case StatusFilterSkipped:
		return "skipped"
	default:
		return "all"
	}
}

// ParseStatusFilter ä»å­—ç¬¦ä¸²è§£æçŠ¶æ€è¿‡æ»¤å™¨
func ParseStatusFilter(s string) StatusFilter {
	switch s {
	case "success":
		return StatusFilterSuccess
	case "failed":
		return StatusFilterFailed
	case "skipped":
		return StatusFilterSkipped
	default:
		return StatusFilterAll
	}
}

// DetailStorageInterface å­˜å‚¨æ¥å£ï¼ˆç»Ÿä¸€ SQLite å’Œ Memory å®ç°ï¼‰
type DetailStorageInterface interface {
	// Write å†™å…¥è¯·æ±‚è¯¦æƒ…
	Write(detail *RequestDetail)

	// Query åˆ†é¡µæŸ¥è¯¢è¯·æ±‚è¯¦æƒ…
	Query(offset, limit int, statusFilter StatusFilter) ([]*RequestDetail, error)

	// Count ç»Ÿè®¡æ€»æ•°
	Count(statusFilter StatusFilter) (int, error)

	// Close å…³é—­å­˜å‚¨å¹¶é‡Šæ”¾èµ„æº
	Close() error

	// GetNodeID è·å–èŠ‚ç‚¹IDï¼ˆç”¨äºåˆ†å¸ƒå¼åœºæ™¯ï¼‰
	GetNodeID() string
}

// DetailStorage SQLiteæŒä¹…åŒ–å­˜å‚¨ï¼ˆå®ç° DetailStorageInterfaceï¼‰
type DetailStorage struct {
	db          *sql.DB
	writeChan   chan *RequestDetail
	batchSize   int
	flushTicker *time.Ticker
	wg          sync.WaitGroup
	closed      bool
	mu          sync.Mutex
	nodeID      string // èŠ‚ç‚¹IDï¼ˆåˆ†å¸ƒå¼æ¨¡å¼ä¸‹æ ‡è¯†æ•°æ®æ¥æºï¼‰
	logger      logger.ILogger

	// ç»Ÿè®¡ä¿¡æ¯
	writeCount    uint64 // å†™å…¥æ€»æ•°
	flushCount    uint64 // åˆ·æ–°æ¬¡æ•°
	dropCount     uint64 // ä¸¢å¼ƒæ•°ï¼ˆé€šé“æ»¡ï¼‰
	lastFlushTime time.Time
}

// NewDetailStorage åˆ›å»ºå­˜å‚¨å®ä¾‹
func NewDetailStorage(dbPath, nodeID string, log logger.ILogger) (*DetailStorage, error) {
	// å¦‚æœä¸æ˜¯å†…å­˜æ¨¡å¼ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
	if dbPath != ":memory:" {
		dir := filepath.Dir(dbPath)
		if err := os.MkdirAll(dir, 0755); err != nil {
			return nil, fmt.Errorf("åˆ›å»ºç›®å½•å¤±è´¥: %w", err)
		}
	}

	db, err := sql.Open("sqlite3", dbPath)
	if err != nil {
		return nil, fmt.Errorf("æ‰“å¼€æ•°æ®åº“å¤±è´¥: %w", err)
	}

	// è®¾ç½®è¿æ¥æ± ï¼ˆSQLite ä»…æ”¯æŒå•å†™å¤šè¯»ï¼‰
	db.SetMaxOpenConns(1)
	db.SetMaxIdleConns(1)
	db.SetConnMaxLifetime(0) // è¿æ¥æ°¸ä¸è¿‡æœŸ

	// ä¼˜åŒ– SQLite æ€§èƒ½
	pragmas := []string{
		"PRAGMA journal_mode = WAL",    // Write-Ahead Logging æ¨¡å¼
		"PRAGMA synchronous = NORMAL",  // å¹³è¡¡æ€§èƒ½å’Œå®‰å…¨æ€§
		"PRAGMA cache_size = 10000",    // 10MB ç¼“å­˜
		"PRAGMA temp_store = MEMORY",   // ä¸´æ—¶è¡¨å­˜å†…å­˜
		"PRAGMA mmap_size = 268435456", // 256MB å†…å­˜æ˜ å°„
	}

	for _, pragma := range pragmas {
		if _, err := db.Exec(pragma); err != nil {
			log.Warnf("âš ï¸  æ‰§è¡Œ %s å¤±è´¥: %v", pragma, err)
		}
	}

	// åˆ›å»ºè¡¨ç»“æ„
	schema := `
	CREATE TABLE IF NOT EXISTS request_details (
		id TEXT PRIMARY KEY,
		node_id TEXT NOT NULL,
		group_id TEXT,
		api_name TEXT,
		timestamp INTEGER NOT NULL,
		url TEXT,
		method TEXT,
		query TEXT,
		headers TEXT,
		body TEXT,
		duration INTEGER NOT NULL,
		status_code INTEGER,
		success INTEGER NOT NULL,
		skipped INTEGER NOT NULL,
		size INTEGER,
		error TEXT,
		response_body TEXT,
		response_headers TEXT,
		verifications TEXT,
		extracted_vars TEXT
	);
	CREATE INDEX IF NOT EXISTS idx_node_id ON request_details(node_id);
	CREATE INDEX IF NOT EXISTS idx_timestamp ON request_details(timestamp);
	CREATE INDEX IF NOT EXISTS idx_success ON request_details(success);
	CREATE INDEX IF NOT EXISTS idx_skipped ON request_details(skipped);
	CREATE INDEX IF NOT EXISTS idx_api_name ON request_details(api_name);
	`

	if _, err := db.Exec(schema); err != nil {
		db.Close()
		return nil, fmt.Errorf("åˆ›å»ºè¡¨å¤±è´¥: %w", err)
	}

	if dbPath != ":memory:" {
		log.Infof("ğŸ’¾ SQLite å­˜å‚¨å·²å¯ç”¨: %s (èŠ‚ç‚¹: %s)", dbPath, nodeID)
	} else {
		log.Infof("ğŸ’¾ SQLite å†…å­˜æ¨¡å¼å·²å¯ç”¨ (èŠ‚ç‚¹: %s)", nodeID)
	}

	storage := &DetailStorage{
		db:            db,
		writeChan:     make(chan *RequestDetail, 10000), // 1ä¸‡ç¼“å†²
		batchSize:     100,                              // æ¯100æ¡æ‰¹é‡å†™å…¥
		flushTicker:   time.NewTicker(1 * time.Second),  // æ¯ç§’å¼ºåˆ¶åˆ·æ–°
		nodeID:        nodeID,
		logger:        log,
		writeCount:    0,
		flushCount:    0,
		dropCount:     0,
		lastFlushTime: time.Now(),
	}

	// å¯åŠ¨å¼‚æ­¥å†™å…¥åç¨‹
	storage.wg.Add(1)
	go storage.batchWriter()

	return storage, nil
}

// Write å¼‚æ­¥å†™å…¥è¯·æ±‚è¯¦æƒ…ï¼ˆå®ç° DetailStorageInterfaceï¼‰
func (s *DetailStorage) Write(detail *RequestDetail) {
	s.mu.Lock()
	if s.closed {
		s.mu.Unlock()
		return
	}
	s.mu.Unlock()

	select {
	case s.writeChan <- detail:
		// å†™å…¥æˆåŠŸ
	default:
		// é€šé“æ»¡äº†ï¼Œä¸¢å¼ƒï¼ˆé¿å…é˜»å¡ä¸»æµç¨‹ï¼‰
		s.dropCount++
		if s.dropCount%1000 == 1 { // æ¯1000æ¬¡ä¸¢å¼ƒè­¦å‘Šä¸€æ¬¡
			s.logger.Warnf("âš ï¸  å†™å…¥é€šé“å·²æ»¡ï¼Œå·²ä¸¢å¼ƒ %d æ¡è®°å½•", s.dropCount)
		}
	}
}

// batchWriter æ‰¹é‡å†™å…¥åç¨‹
func (s *DetailStorage) batchWriter() {
	defer s.wg.Done()

	batch := make([]*RequestDetail, 0, s.batchSize)

	flush := func() {
		if len(batch) == 0 {
			return
		}

		start := time.Now()
		if err := s.insertBatch(batch); err != nil {
			// å†™å…¥å¤±è´¥ï¼Œè®°å½•æ—¥å¿—ä½†ä¸é˜»å¡
			s.logger.Errorf("âŒ æ‰¹é‡å†™å…¥ %d æ¡è®°å½•å¤±è´¥: %v", len(batch), err)
		} else {
			s.writeCount += uint64(len(batch))
			s.flushCount++
			s.lastFlushTime = time.Now()

			// æ¯å†™å…¥10000æ¡è®°å½•è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
			if s.writeCount%10000 == 0 {
				duration := time.Since(start)
				s.logger.Debugf("ğŸ“Š å·²å†™å…¥ %d æ¡è®°å½• (æœ¬æ¬¡: %d æ¡, è€—æ—¶: %v)",
					s.writeCount, len(batch), duration)
			}
		}

		batch = batch[:0] // æ¸…ç©ºä½†ä¿ç•™å®¹é‡
	}

	for {
		select {
		case detail, ok := <-s.writeChan:
			if !ok {
				// é€šé“å…³é—­ï¼Œåˆ·æ–°å‰©ä½™æ•°æ®
				flush()
				return
			}

			batch = append(batch, detail)
			if len(batch) >= s.batchSize {
				flush()
			}

		case <-s.flushTicker.C:
			// å®šæ—¶åˆ·æ–°
			flush()
		}
	}
}

// insertBatch æ‰¹é‡æ’å…¥
func (s *DetailStorage) insertBatch(batch []*RequestDetail) error {
	tx, err := s.db.Begin()
	if err != nil {
		return err
	}
	defer tx.Rollback()

	stmt, err := tx.Prepare(`
		INSERT INTO request_details (
			id, node_id, group_id, api_name, timestamp, url, method, query, headers, body,
			duration, status_code, success, skipped, size, error,
			response_body, response_headers, verifications, extracted_vars
		) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
	`)
	if err != nil {
		return err
	}
	defer stmt.Close()

	for _, detail := range batch {
		// åºåˆ—åŒ–å¤æ‚å­—æ®µ
		headersJSON, _ := json.Marshal(detail.Headers)
		respHeadersJSON, _ := json.Marshal(detail.ResponseHeaders)
		verificationsJSON, _ := json.Marshal(detail.Verifications)
		extractedVarsJSON, _ := json.Marshal(detail.ExtractedVars)

		_, err := stmt.Exec(
			detail.ID,
			s.nodeID, // è®°å½•èŠ‚ç‚¹ID
			detail.GroupID,
			detail.APIName,
			detail.Timestamp.Unix(),
			detail.URL,
			detail.Method,
			detail.Query,
			string(headersJSON),
			detail.Body,
			detail.Duration.Microseconds(),
			detail.StatusCode,
			boolToInt(detail.Success),
			boolToInt(detail.Skipped),
			detail.Size,
			detail.Error,
			detail.ResponseBody,
			string(respHeadersJSON),
			string(verificationsJSON),
			string(extractedVarsJSON),
		)
		if err != nil {
			return err
		}
	}

	return tx.Commit()
}

// Query åˆ†é¡µæŸ¥è¯¢è¯·æ±‚è¯¦æƒ…
func (s *DetailStorage) Query(offset, limit int, statusFilter StatusFilter) ([]*RequestDetail, error) {
	query := "SELECT * FROM request_details"

	// æ ¹æ®çŠ¶æ€è¿‡æ»¤
	switch statusFilter {
	case StatusFilterSuccess:
		query += " WHERE success = 1 AND skipped = 0"
	case StatusFilterFailed:
		query += " WHERE success = 0 AND skipped = 0"
	case StatusFilterSkipped:
		query += " WHERE skipped = 1"
	case StatusFilterAll:
	default:
		// é»˜è®¤æŸ¥è¯¢å…¨éƒ¨
	}

	query += " ORDER BY id DESC LIMIT ? OFFSET ?"

	rows, err := s.db.Query(query, limit, offset)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var results []*RequestDetail
	for rows.Next() {
		detail, err := s.scanDetail(rows)
		if err != nil {
			continue
		}
		results = append(results, detail)
	}

	return results, nil
}

// Count ç»Ÿè®¡æ€»æ•°
func (s *DetailStorage) Count(statusFilter StatusFilter) (int, error) {
	query := "SELECT COUNT(*) FROM request_details"

	// æ ¹æ®çŠ¶æ€è¿‡æ»¤
	switch statusFilter {
	case StatusFilterSuccess:
		query += " WHERE success = 1 AND skipped = 0"
	case StatusFilterFailed:
		query += " WHERE success = 0 AND skipped = 0"
	case StatusFilterSkipped:
		query += " WHERE skipped = 1"
	case StatusFilterAll:
		// ä¸æ·»åŠ è¿‡æ»¤æ¡ä»¶
	default:
		// é»˜è®¤æŸ¥è¯¢å…¨éƒ¨
	}

	var count int
	err := s.db.QueryRow(query).Scan(&count)
	return count, err
}

// scanDetail æ‰«æè¡Œæ•°æ®
func (s *DetailStorage) scanDetail(rows *sql.Rows) (*RequestDetail, error) {
	var (
		detail                               RequestDetail
		nodeID                               string
		timestamp, duration                  int64
		success, skipped                     int
		headersJSON, respHeadersJSON         string
		verificationsJSON, extractedVarsJSON string
	)

	err := rows.Scan(
		&detail.ID, &nodeID, &detail.GroupID, &detail.APIName, &timestamp,
		&detail.URL, &detail.Method, &detail.Query, &headersJSON, &detail.Body,
		&duration, &detail.StatusCode, &success, &skipped, &detail.Size, &detail.Error,
		&detail.ResponseBody, &respHeadersJSON, &verificationsJSON, &extractedVarsJSON,
	)
	if err != nil {
		return nil, err
	}

	detail.Timestamp = time.Unix(timestamp, 0)
	detail.Duration = time.Duration(duration) * time.Microsecond
	detail.Success = success == 1
	detail.Skipped = skipped == 1

	json.Unmarshal([]byte(headersJSON), &detail.Headers)
	json.Unmarshal([]byte(respHeadersJSON), &detail.ResponseHeaders)
	json.Unmarshal([]byte(verificationsJSON), &detail.Verifications)
	json.Unmarshal([]byte(extractedVarsJSON), &detail.ExtractedVars)

	return &detail, nil
}

// Close å…³é—­å­˜å‚¨
func (s *DetailStorage) Close() error {
	s.mu.Lock()
	if s.closed {
		s.mu.Unlock()
		return nil
	}
	s.closed = true
	s.mu.Unlock()

	// å…³é—­å†™å…¥é€šé“ï¼Œè§¦å‘ batchWriter åˆ·æ–°å‰©ä½™æ•°æ®
	close(s.writeChan)

	// åœæ­¢å®šæ—¶å™¨
	s.flushTicker.Stop()

	// ç­‰å¾… batchWriter å®Œæˆï¼ˆä¼šè‡ªåŠ¨åˆ·æ–°å‰©ä½™æ•°æ®ï¼‰
	s.wg.Wait()

	// è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
	s.logger.Infof("âœ… SQLite å­˜å‚¨å·²å…³é—­")
	s.logger.Infof("   ğŸ“ æ€»å†™å…¥: %d æ¡", s.writeCount)
	s.logger.Infof("   ğŸ”„ åˆ·æ–°æ¬¡æ•°: %d æ¬¡", s.flushCount)
	if s.dropCount > 0 {
		s.logger.Warnf("   âš ï¸  ä¸¢å¼ƒè®°å½•: %d æ¡", s.dropCount)
	}

	return s.db.Close()
}

// GetNodeID è·å–èŠ‚ç‚¹IDï¼ˆå®ç° DetailStorageInterfaceï¼‰
func (s *DetailStorage) GetNodeID() string {
	return s.nodeID
}

// GetStats è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
func (s *DetailStorage) GetStats() (writeCount, flushCount, dropCount uint64) {
	return s.writeCount, s.flushCount, s.dropCount
}

func boolToInt(b bool) int {
	if b {
		return 1
	}
	return 0
}
